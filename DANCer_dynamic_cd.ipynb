{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import urllib.request as urllib\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "import time\n",
    "import datetime \n",
    "import dynetx as dn\n",
    "import copy\n",
    "import ast\n",
    "import glob\n",
    "import scipy\n",
    "import community as community_louvain\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.linalg import diag\n",
    "from tensorflow.keras import callbacks\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from itertools import count\n",
    "from rdyn import RDyn\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from helpers import *\n",
    "from Autoencoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './graphs/DANCer_graphs_simple_4_built/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "for i in range(5):\n",
    "    graphs.append(pickle.load(open(path + 't' + str(i) + '.p', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 data\n",
    "t1 = graphs[0]\n",
    "\n",
    "t1_A = create_adjacency_matrix(t1)\n",
    "t1_B = tf.convert_to_tensor(nx.modularity_matrix(t1).astype('float32'))\n",
    "t1_S = tf.convert_to_tensor(adjacency_to_similarity(t1_A).astype('float32'))\n",
    "t1_T = tf.convert_to_tensor(probability_transition_matrix(t1_A, 4))\n",
    "t1_P = tf.convert_to_tensor(proximity_matrix(t1, 0.5).astype('float32'))\n",
    "\n",
    "F = create_feature_matrix(t1, 5)\n",
    "C = cosine_similarity(F)\n",
    "C = top_k(C, int(average_community_size(graph_labels(t1))))\n",
    "t1_M = tf.convert_to_tensor(markov_matrix(t1_A, C).astype('float32'))\n",
    "\n",
    "\n",
    "t2 = graphs[1]\n",
    "\n",
    "t2_A = create_adjacency_matrix(t2)\n",
    "t2_B = tf.convert_to_tensor(nx.modularity_matrix(t2).astype('float32'))\n",
    "t2_S = tf.convert_to_tensor(adjacency_to_similarity(t2_A).astype('float32'))\n",
    "t2_T = tf.convert_to_tensor(probability_transition_matrix(t2_A, 4))\n",
    "t2_P = tf.convert_to_tensor(proximity_matrix(t2, 0.5).astype('float32'))\n",
    "\n",
    "F = create_feature_matrix(t2, 5)\n",
    "C = cosine_similarity(F)\n",
    "C = top_k(C, int(average_community_size(graph_labels(t2))))\n",
    "t2_M = tf.convert_to_tensor(markov_matrix(t2_A, C).astype('float32'))\n",
    "\n",
    "\n",
    "t3 = graphs[2]\n",
    "\n",
    "t3_A = create_adjacency_matrix(t3)\n",
    "t3_B = tf.convert_to_tensor(nx.modularity_matrix(t3).astype('float32'))\n",
    "t3_S = tf.convert_to_tensor(adjacency_to_similarity(t3_A).astype('float32'))\n",
    "t3_T = tf.convert_to_tensor(probability_transition_matrix(t3_A, 4))\n",
    "t3_P = tf.convert_to_tensor(proximity_matrix(t3, 0.5).astype('float32'))\n",
    "\n",
    "F = create_feature_matrix(t3, 5)\n",
    "C = cosine_similarity(F)\n",
    "C = top_k(C, int(average_community_size(graph_labels(t3))))\n",
    "t3_M = tf.convert_to_tensor(markov_matrix(t3_A, C).astype('float32'))\n",
    "\n",
    "\n",
    "t4 = graphs[3]\n",
    "\n",
    "t4_A = create_adjacency_matrix(t4)\n",
    "t4_B = tf.convert_to_tensor(nx.modularity_matrix(t4).astype('float32'))\n",
    "t4_S = tf.convert_to_tensor(adjacency_to_similarity(t4_A).astype('float32'))\n",
    "t4_T = tf.convert_to_tensor(probability_transition_matrix(t4_A, 4))\n",
    "t4_P = tf.convert_to_tensor(proximity_matrix(t4, 0.5).astype('float32'))\n",
    "\n",
    "F = create_feature_matrix(t4, 5)\n",
    "C = cosine_similarity(F)\n",
    "C = top_k(C, int(average_community_size(graph_labels(t4))))\n",
    "t4_M = tf.convert_to_tensor(markov_matrix(t4_A, C).astype('float32'))\n",
    "\n",
    "\n",
    "t5 = graphs[1]\n",
    "\n",
    "t5_A = create_adjacency_matrix(t5)\n",
    "t5_B = tf.convert_to_tensor(nx.modularity_matrix(t5).astype('float32'))\n",
    "t5_S = tf.convert_to_tensor(adjacency_to_similarity(t5_A).astype('float32'))\n",
    "t5_T = tf.convert_to_tensor(probability_transition_matrix(t5_A, 4))\n",
    "t5_P = tf.convert_to_tensor(proximity_matrix(t5, 0.5).astype('float32'))\n",
    "\n",
    "F = create_feature_matrix(t5, 5)\n",
    "C = cosine_similarity(F)\n",
    "C = top_k(C, int(average_community_size(graph_labels(t5))))\n",
    "t5_M = tf.convert_to_tensor(markov_matrix(t5_A, C).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save input matrices\n",
    "pickle.dump(t1_A, open('./trained/DANCer_graphs_simple_4/input_matrices/t1_A', 'wb'))\n",
    "pickle.dump(t1_B, open('./trained/DANCer_graphs_simple_4/input_matrices/t1_B', 'wb'))\n",
    "pickle.dump(t1_S, open('./trained/DANCer_graphs_simple_4/input_matrices/t1_S', 'wb'))\n",
    "pickle.dump(t1_T, open('./trained/DANCer_graphs_simple_4/input_matrices/t1_T', 'wb'))\n",
    "pickle.dump(t1_P, open('./trained/DANCer_graphs_simple_4/input_matrices/t1_P', 'wb'))\n",
    "pickle.dump(t1_M, open('./trained/DANCer_graphs_simple_4/input_matrices/t1_M', 'wb'))\n",
    "\n",
    "pickle.dump(t2_A, open('./trained/DANCer_graphs_simple_4/input_matrices/t2_A', 'wb'))\n",
    "pickle.dump(t2_B, open('./trained/DANCer_graphs_simple_4/input_matrices/t2_B', 'wb'))\n",
    "pickle.dump(t2_S, open('./trained/DANCer_graphs_simple_4/input_matrices/t2_S', 'wb'))\n",
    "pickle.dump(t2_T, open('./trained/DANCer_graphs_simple_4/input_matrices/t2_T', 'wb'))\n",
    "pickle.dump(t2_P, open('./trained/DANCer_graphs_simple_4/input_matrices/t2_P', 'wb'))\n",
    "pickle.dump(t2_M, open('./trained/DANCer_graphs_simple_4/input_matrices/t2_M', 'wb'))\n",
    "\n",
    "pickle.dump(t3_A, open('./trained/DANCer_graphs_simple_4/input_matrices/t3_A', 'wb'))\n",
    "pickle.dump(t3_B, open('./trained/DANCer_graphs_simple_4/input_matrices/t3_B', 'wb'))\n",
    "pickle.dump(t3_S, open('./trained/DANCer_graphs_simple_4/input_matrices/t3_S', 'wb'))\n",
    "pickle.dump(t3_T, open('./trained/DANCer_graphs_simple_4/input_matrices/t3_T', 'wb'))\n",
    "pickle.dump(t3_P, open('./trained/DANCer_graphs_simple_4/input_matrices/t3_P', 'wb'))\n",
    "pickle.dump(t3_M, open('./trained/DANCer_graphs_simple_4/input_matrices/t3_M', 'wb'))\n",
    "\n",
    "pickle.dump(t4_A, open('./trained/DANCer_graphs_simple_4/input_matrices/t4_A', 'wb'))\n",
    "pickle.dump(t4_B, open('./trained/DANCer_graphs_simple_4/input_matrices/t4_B', 'wb'))\n",
    "pickle.dump(t4_S, open('./trained/DANCer_graphs_simple_4/input_matrices/t4_S', 'wb'))\n",
    "pickle.dump(t4_T, open('./trained/DANCer_graphs_simple_4/input_matrices/t4_T', 'wb'))\n",
    "pickle.dump(t4_P, open('./trained/DANCer_graphs_simple_4/input_matrices/t4_P', 'wb'))\n",
    "pickle.dump(t4_M, open('./trained/DANCer_graphs_simple_4/input_matrices/t4_M', 'wb'))\n",
    "\n",
    "pickle.dump(t5_A, open('./trained/DANCer_graphs_simple_4/input_matrices/t5_A', 'wb'))\n",
    "pickle.dump(t5_B, open('./trained/DANCer_graphs_simple_4/input_matrices/t5_B', 'wb'))\n",
    "pickle.dump(t5_S, open('./trained/DANCer_graphs_simple_4/input_matrices/t5_S', 'wb'))\n",
    "pickle.dump(t5_T, open('./trained/DANCer_graphs_simple_4/input_matrices/t5_T', 'wb'))\n",
    "pickle.dump(t5_P, open('./trained/DANCer_graphs_simple_4/input_matrices/t5_P', 'wb'))\n",
    "pickle.dump(t5_M, open('./trained/DANCer_graphs_simple_4/input_matrices/t5_M', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize graph adjacency structure\n",
    "n0 = [x for x,y in t1.nodes(data=True) if y['gt']==0]\n",
    "n1 = [x for x,y in t1.nodes(data=True) if y['gt']==1]\n",
    "n2 = [x for x,y in t1.nodes(data=True) if y['gt']==2]\n",
    "n3 = [x for x,y in t1.nodes(data=True) if y['gt']==3]\n",
    "n4 = [x for x,y in t1.nodes(data=True) if y['gt']==4]\n",
    "n5 = [x for x,y in t1.nodes(data=True) if y['gt']==5]\n",
    "n6 = [x for x,y in t1.nodes(data=True) if y['gt']==6]\n",
    "n7 = [x for x,y in t1.nodes(data=True) if y['gt']==7]\n",
    "n8 = [x for x,y in t1.nodes(data=True) if y['gt']==8]\n",
    "n9 = [x for x,y in t1.nodes(data=True) if y['gt']==9]\n",
    "\n",
    "\n",
    "n = n0 + n1 + n2 + n3 + n4 + n5 + n6 + n7 + n8 + n9\n",
    "plt.matshow(nx.to_numpy_matrix(t1, dtype=np.bool, nodelist=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_static_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t1_static_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t1_static_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t1_static_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "\n",
    "t2_static_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t2_static_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t2_static_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t2_static_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "\n",
    "t3_static_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t3_static_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t3_static_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t3_static_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "\n",
    "t4_static_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t4_static_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t4_static_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t4_static_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "\n",
    "t5_static_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t5_static_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t5_static_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())\n",
    "t5_static_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t1_static_S = train(t1_static_S, 1000, 300, t1_S)\n",
    "history_t1_static_B = train(t1_static_B, 1000, 300, t1_B)\n",
    "history_t1_static_T = train(t1_static_T, 1000, 300, t1_T)\n",
    "history_t1_static_P = train(t1_static_P, 1000, 300, t1_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t2_static_S = train(t2_static_S, 1000, 300, t2_S)\n",
    "history_t2_static_B = train(t2_static_B, 1000, 300, t2_B)\n",
    "history_t2_static_T = train(t2_static_T, 1000, 300, t2_T)\n",
    "history_t2_static_P = train(t2_static_P, 1000, 300, t2_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t3_static_S = train(t3_static_S, 1000, 300, t3_S)\n",
    "history_t3_static_B = train(t3_static_B, 1000, 300, t3_B)\n",
    "history_t3_static_T = train(t3_static_T, 1000, 300, t3_T)\n",
    "history_t3_static_P = train(t3_static_P, 1000, 300, t3_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t4_static_S = train(t4_static_S, 1000, 300, t4_S)\n",
    "history_t4_static_B = train(t4_static_B, 1000, 300, t4_B)\n",
    "history_t4_static_T = train(t4_static_T, 1000, 300, t4_T)\n",
    "history_t4_static_P = train(t4_static_P, 1000, 300, t4_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t5_static_S = train(t5_static_S, 1000, 300, t5_S)\n",
    "history_t5_static_B = train(t5_static_B, 1000, 300, t5_B)\n",
    "history_t5_static_T = train(t5_static_T, 1000, 300, t5_T)\n",
    "history_t5_static_P = train(t5_static_P, 1000, 300, t5_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_S_H = t1_static_S.encoder(t1_S)\n",
    "t1_B_H = t1_static_B.encoder(t1_B)\n",
    "t1_T_H = t1_static_T.encoder(t1_T)\n",
    "t1_P_H = t1_static_P.encoder(t1_P)\n",
    "\n",
    "t2_S_H = t2_static_S.encoder(t2_S)\n",
    "t2_B_H = t2_static_B.encoder(t2_B)\n",
    "t2_T_H = t2_static_T.encoder(t2_T)\n",
    "t2_P_H = t2_static_P.encoder(t2_P)\n",
    "\n",
    "t3_S_H = t3_static_S.encoder(t3_S)\n",
    "t3_B_H = t3_static_B.encoder(t3_B)\n",
    "t3_T_H = t3_static_T.encoder(t3_T)\n",
    "t3_P_H = t3_static_P.encoder(t3_P)\n",
    "\n",
    "t4_S_H = t4_static_S.encoder(t4_S)\n",
    "t4_B_H = t4_static_B.encoder(t4_B)\n",
    "t4_T_H = t4_static_T.encoder(t4_T)\n",
    "t4_P_H = t4_static_P.encoder(t4_P)\n",
    "\n",
    "t5_S_H = t5_static_S.encoder(t5_S)\n",
    "t5_B_H = t5_static_B.encoder(t5_B)\n",
    "t5_T_H = t5_static_T.encoder(t5_T)\n",
    "t5_P_H = t5_static_P.encoder(t5_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save latent space representations\n",
    "pickle.dump(t1_S_H, open('./trained/DANCer_graphs_simple_4/latent_space/t1_S_H', 'wb'))\n",
    "pickle.dump(t1_B_H, open('./trained/DANCer_graphs_simple_4/latent_space/t1_B_H', 'wb'))\n",
    "pickle.dump(t1_T_H, open('./trained/DANCer_graphs_simple_4/latent_space/t1_T_H', 'wb'))\n",
    "pickle.dump(t1_P_H, open('./trained/DANCer_graphs_simple_4/latent_space/t1_P_H', 'wb'))\n",
    "\n",
    "pickle.dump(t2_S_H, open('./trained/DANCer_graphs_simple_4/latent_space/t2_S_H', 'wb'))\n",
    "pickle.dump(t2_B_H, open('./trained/DANCer_graphs_simple_4/latent_space/t2_B_H', 'wb'))\n",
    "pickle.dump(t2_T_H, open('./trained/DANCer_graphs_simple_4/latent_space/t2_T_H', 'wb'))\n",
    "pickle.dump(t2_P_H, open('./trained/DANCer_graphs_simple_4/latent_space/t2_P_H', 'wb'))\n",
    "\n",
    "pickle.dump(t3_S_H, open('./trained/DANCer_graphs_simple_4/latent_space/t3_S_H', 'wb'))\n",
    "pickle.dump(t3_B_H, open('./trained/DANCer_graphs_simple_4/latent_space/t3_B_H', 'wb'))\n",
    "pickle.dump(t3_T_H, open('./trained/DANCer_graphs_simple_4/latent_space/t3_T_H', 'wb'))\n",
    "pickle.dump(t3_P_H, open('./trained/DANCer_graphs_simple_4/latent_space/t3_P_H', 'wb'))\n",
    "\n",
    "pickle.dump(t4_S_H, open('./trained/DANCer_graphs_simple_4/latent_space/t4_S_H', 'wb'))\n",
    "pickle.dump(t4_B_H, open('./trained/DANCer_graphs_simple_4/latent_space/t4_B_H', 'wb'))\n",
    "pickle.dump(t4_T_H, open('./trained/DANCer_graphs_simple_4/latent_space/t4_T_H', 'wb'))\n",
    "pickle.dump(t4_P_H, open('./trained/DANCer_graphs_simple_4/latent_space/t4_P_H', 'wb'))\n",
    "\n",
    "pickle.dump(t5_S_H, open('./trained/DANCer_graphs_simple_4/latent_space/t5_S_H', 'wb'))\n",
    "pickle.dump(t5_B_H, open('./trained/DANCer_graphs_simple_4/latent_space/t5_B_H', 'wb'))\n",
    "pickle.dump(t5_T_H, open('./trained/DANCer_graphs_simple_4/latent_space/t5_T_H', 'wb'))\n",
    "pickle.dump(t5_P_H, open('./trained/DANCer_graphs_simple_4/latent_space/t5_P_H', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = t1_static_S.predict(t1_S)\n",
    "temp = t1_static_B.predict(t1_B)\n",
    "temp = t1_static_T.predict(t1_T)\n",
    "temp = t1_static_P.predict(t1_P)\n",
    "\n",
    "temp = t2_static_S.predict(t2_S)\n",
    "temp = t2_static_B.predict(t2_B)\n",
    "temp = t2_static_T.predict(t2_T)\n",
    "temp = t2_static_P.predict(t2_P)\n",
    "\n",
    "temp = t3_static_S.predict(t3_S)\n",
    "temp = t3_static_B.predict(t3_B)\n",
    "temp = t3_static_T.predict(t3_T)\n",
    "temp = t3_static_P.predict(t3_P)\n",
    "\n",
    "temp = t4_static_S.predict(t4_S)\n",
    "temp = t4_static_B.predict(t4_B)\n",
    "temp = t4_static_T.predict(t4_T)\n",
    "temp = t4_static_P.predict(t4_P)\n",
    "\n",
    "temp = t5_static_S.predict(t5_S)\n",
    "temp = t5_static_B.predict(t5_B)\n",
    "temp = t5_static_T.predict(t5_T)\n",
    "temp = t5_static_P.predict(t5_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save models\n",
    "\n",
    "t1_static_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t1_static_S')\n",
    "t1_static_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t1_static_B')\n",
    "t1_static_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t1_static_T')\n",
    "t1_static_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t1_static_P')\n",
    "\n",
    "t2_static_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_static_S')\n",
    "t2_static_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_static_B')\n",
    "t2_static_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_static_T')\n",
    "t2_static_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_static_P')\n",
    "\n",
    "t3_static_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_static_S')\n",
    "t3_static_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_static_B')\n",
    "t3_static_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_static_T')\n",
    "t3_static_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_static_P')\n",
    "\n",
    "t4_static_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_static_S')\n",
    "t4_static_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_static_B')\n",
    "t4_static_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_static_T')\n",
    "t4_static_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_static_P')\n",
    "\n",
    "t5_static_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_static_S')\n",
    "t5_static_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_static_B')\n",
    "t5_static_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_static_T')\n",
    "t5_static_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_static_P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels from static CD clustering\n",
    "t1_static_S_labels = KMeans(n_clusters=get_num_communities(t1, 'gt'), n_init=20).fit(t1_S).labels_\n",
    "t1_static_B_labels = KMeans(n_clusters=get_num_communities(t1, 'gt'), n_init=20).fit(t1_B).labels_\n",
    "t1_static_T_labels = KMeans(n_clusters=get_num_communities(t1, 'gt'), n_init=20).fit(t1_T).labels_\n",
    "t1_static_P_labels = KMeans(n_clusters=get_num_communities(t1, 'gt'), n_init=20).fit(t1_P).labels_\n",
    "\n",
    "#community indicator matrices from static CD clustering\n",
    "t1_static_S_Z = create_community_indicator_matrix(labels_to_dict(t1_static_S_labels), len(t1.nodes))\n",
    "t1_static_B_Z = create_community_indicator_matrix(labels_to_dict(t1_static_B_labels), len(t1.nodes))\n",
    "t1_static_T_Z = create_community_indicator_matrix(labels_to_dict(t1_static_T_labels), len(t1.nodes))\n",
    "t1_static_P_Z = create_community_indicator_matrix(labels_to_dict(t1_static_P_labels), len(t1.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_dynamic_chordal_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t2_dynamic_indicator_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t1_static_S_Z)\n",
    "t2_dynamic_chordal_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t2_dynamic_indicator_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t1_static_B_Z)\n",
    "t2_dynamic_chordal_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t2_dynamic_indicator_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t1_static_T_Z)\n",
    "t2_dynamic_chordal_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t2_dynamic_indicator_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t1_static_P_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_dynamic_chordal_S.set_past_embedding(t1_S_H)\n",
    "t2_dynamic_chordal_B.set_past_embedding(t1_B_H)\n",
    "t2_dynamic_chordal_T.set_past_embedding(t1_T_H)\n",
    "t2_dynamic_chordal_P.set_past_embedding(t1_P_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t2_dynamic_chordal_S = train(t2_dynamic_chordal_S, 1000, 300, t2_S)\n",
    "history_t2_dynamic_chordal_B = train(t2_dynamic_chordal_B, 1000, 300, t2_B)\n",
    "history_t2_dynamic_chordal_T = train(t2_dynamic_chordal_T, 1000, 300, t2_T)\n",
    "history_t2_dynamic_chordal_P = train(t2_dynamic_chordal_P, 1000, 300, t2_P)\n",
    "\n",
    "history_t2_dynamic_indicator_S = train(t2_dynamic_indicator_S, 1000, 300, t2_S)\n",
    "history_t2_dynamic_indicator_B = train(t2_dynamic_indicator_B, 1000, 300, t2_B)\n",
    "history_t2_dynamic_indicator_T = train(t2_dynamic_indicator_T, 1000, 300, t2_T)\n",
    "history_t2_dynamic_indicator_P = train(t2_dynamic_indicator_P, 1000, 300, t2_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_S_H_dynamic_chordal = t2_dynamic_chordal_S.encoder(t2_S)\n",
    "t2_S_H_dynamic_indicator = t2_dynamic_indicator_S.encoder(t2_S)\n",
    "\n",
    "t2_B_H_dynamic_chordal = t2_dynamic_chordal_B.encoder(t2_B)\n",
    "t2_B_H_dynamic_indicator = t2_dynamic_indicator_B.encoder(t2_B)\n",
    "\n",
    "t2_T_H_dynamic_chordal = t2_dynamic_chordal_T.encoder(t2_T)\n",
    "t2_T_H_dynamic_indicator = t2_dynamic_indicator_T.encoder(t2_T)\n",
    "\n",
    "t2_P_H_dynamic_chordal = t2_dynamic_chordal_P.encoder(t2_P)\n",
    "t2_P_H_dynamic_indicator = t2_dynamic_indicator_P.encoder(t2_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new dynamic latent space representations\n",
    "pickle.dump(t2_S_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t2_S_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t2_B_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t2_B_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t2_T_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t2_T_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t2_P_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t2_P_H_dynamic_chordal', 'wb'))\n",
    "\n",
    "pickle.dump(t2_S_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t2_S_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t2_B_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t2_B_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t2_T_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t2_T_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t2_P_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t2_P_H_dynamic_indicator', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_dynamic_S_labels = KMeans(n_clusters=get_num_communities(t2, 'gt'), n_init=20).fit(t2_S_H_dynamic_indicator).labels_\n",
    "t2_dynamic_B_labels = KMeans(n_clusters=get_num_communities(t2, 'gt'), n_init=20).fit(t2_B_H_dynamic_indicator).labels_\n",
    "t2_dynamic_T_labels = KMeans(n_clusters=get_num_communities(t2, 'gt'), n_init=20).fit(t2_T_H_dynamic_indicator).labels_\n",
    "t2_dynamic_P_labels = KMeans(n_clusters=get_num_communities(t2, 'gt'), n_init=20).fit(t2_P_H_dynamic_indicator).labels_\n",
    "\n",
    "t2_dynamic_S_Z = create_community_indicator_matrix(labels_to_dict(t2_dynamic_S_labels), len(t2.nodes))\n",
    "t2_dynamic_B_Z = create_community_indicator_matrix(labels_to_dict(t2_dynamic_B_labels), len(t2.nodes))\n",
    "t2_dynamic_T_Z = create_community_indicator_matrix(labels_to_dict(t2_dynamic_T_labels), len(t2.nodes))\n",
    "t2_dynamic_P_Z = create_community_indicator_matrix(labels_to_dict(t2_dynamic_P_labels), len(t2.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_dynamic_chordal_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t3_dynamic_indicator_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t2_dynamic_S_Z)\n",
    "t3_dynamic_chordal_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t3_dynamic_indicator_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t2_dynamic_B_Z)\n",
    "t3_dynamic_chordal_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t3_dynamic_indicator_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t2_dynamic_T_Z)\n",
    "t3_dynamic_chordal_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t3_dynamic_indicator_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t2_dynamic_P_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_dynamic_chordal_S.set_past_embedding(t2_S_H_dynamic_chordal)\n",
    "t3_dynamic_chordal_B.set_past_embedding(t2_B_H_dynamic_chordal)\n",
    "t3_dynamic_chordal_T.set_past_embedding(t2_T_H_dynamic_chordal)\n",
    "t3_dynamic_chordal_P.set_past_embedding(t2_P_H_dynamic_chordal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t3_dynamic_chordal_S = train(t3_dynamic_chordal_S, 1000, 300, t3_S)\n",
    "history_t3_dynamic_chordal_B = train(t3_dynamic_chordal_B, 1000, 300, t3_B)\n",
    "history_t3_dynamic_chordal_T = train(t3_dynamic_chordal_T, 1000, 300, t3_T)\n",
    "history_t3_dynamic_chordal_P = train(t3_dynamic_chordal_P, 1000, 300, t3_P)\n",
    "\n",
    "history_t3_dynamic_indicator_S = train(t3_dynamic_indicator_S, 1000, 300, t3_S)\n",
    "history_t3_dynamic_indicator_B = train(t3_dynamic_indicator_B, 1000, 300, t3_B)\n",
    "history_t3_dynamic_indicator_T = train(t3_dynamic_indicator_T, 1000, 300, t3_T)\n",
    "history_t3_dynamic_indicator_P = train(t3_dynamic_indicator_P, 1000, 300, t3_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_S_H_dynamic_chordal = t3_dynamic_chordal_S.encoder(t3_S)\n",
    "t3_S_H_dynamic_indicator = t3_dynamic_indicator_S.encoder(t3_S)\n",
    "\n",
    "t3_B_H_dynamic_chordal = t3_dynamic_chordal_B.encoder(t3_B)\n",
    "t3_B_H_dynamic_indicator = t3_dynamic_indicator_B.encoder(t3_B)\n",
    "\n",
    "t3_T_H_dynamic_chordal = t3_dynamic_chordal_T.encoder(t3_T)\n",
    "t3_T_H_dynamic_indicator = t3_dynamic_indicator_T.encoder(t3_T)\n",
    "\n",
    "t3_P_H_dynamic_chordal = t3_dynamic_chordal_P.encoder(t3_P)\n",
    "t3_P_H_dynamic_indicator = t3_dynamic_indicator_P.encoder(t3_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new dynamic latent space representations\n",
    "pickle.dump(t3_S_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t3_S_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t3_B_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t3_B_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t3_T_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t3_T_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t3_P_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t3_P_H_dynamic_chordal', 'wb'))\n",
    "\n",
    "pickle.dump(t3_S_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t3_S_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t3_B_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t3_B_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t3_T_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t3_T_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t3_P_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t3_P_H_dynamic_indicator', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_dynamic_S_labels = KMeans(n_clusters=get_num_communities(t3, 'gt'), n_init=20).fit(t3_S_H_dynamic_indicator).labels_\n",
    "t3_dynamic_B_labels = KMeans(n_clusters=get_num_communities(t3, 'gt'), n_init=20).fit(t3_B_H_dynamic_indicator).labels_\n",
    "t3_dynamic_T_labels = KMeans(n_clusters=get_num_communities(t3, 'gt'), n_init=20).fit(t3_T_H_dynamic_indicator).labels_\n",
    "t3_dynamic_P_labels = KMeans(n_clusters=get_num_communities(t3, 'gt'), n_init=20).fit(t3_P_H_dynamic_indicator).labels_\n",
    "\n",
    "t3_dynamic_S_Z = create_community_indicator_matrix(labels_to_dict(t3_dynamic_S_labels), len(t3.nodes))\n",
    "t3_dynamic_B_Z = create_community_indicator_matrix(labels_to_dict(t3_dynamic_B_labels), len(t3.nodes))\n",
    "t3_dynamic_T_Z = create_community_indicator_matrix(labels_to_dict(t3_dynamic_T_labels), len(t3.nodes))\n",
    "t3_dynamic_P_Z = create_community_indicator_matrix(labels_to_dict(t3_dynamic_P_labels), len(t3.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4_dynamic_chordal_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t4_dynamic_indicator_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t3_dynamic_S_Z)\n",
    "t4_dynamic_chordal_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t4_dynamic_indicator_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t3_dynamic_B_Z)\n",
    "t4_dynamic_chordal_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t4_dynamic_indicator_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t3_dynamic_T_Z)\n",
    "t4_dynamic_chordal_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t4_dynamic_indicator_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t3_dynamic_P_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4_dynamic_chordal_S.set_past_embedding(t3_S_H_dynamic_chordal)\n",
    "t4_dynamic_chordal_B.set_past_embedding(t3_B_H_dynamic_chordal)\n",
    "t4_dynamic_chordal_T.set_past_embedding(t3_T_H_dynamic_chordal)\n",
    "t4_dynamic_chordal_P.set_past_embedding(t3_P_H_dynamic_chordal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t4_dynamic_chordal_S = train(t4_dynamic_chordal_S, 1000, 300, t4_S)\n",
    "history_t4_dynamic_chordal_B = train(t4_dynamic_chordal_B, 1000, 300, t4_B)\n",
    "history_t4_dynamic_chordal_T = train(t4_dynamic_chordal_T, 1000, 300, t4_T)\n",
    "history_t4_dynamic_chordal_P = train(t4_dynamic_chordal_P, 1000, 300, t4_P)\n",
    "\n",
    "history_t4_dynamic_indicator_S = train(t4_dynamic_indicator_S, 1000, 300, t4_S)\n",
    "history_t4_dynamic_indicator_B = train(t4_dynamic_indicator_B, 1000, 300, t4_B)\n",
    "history_t4_dynamic_indicator_T = train(t4_dynamic_indicator_T, 1000, 300, t4_T)\n",
    "history_t4_dynamic_indicator_P = train(t4_dynamic_indicator_P, 1000, 300, t4_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4_S_H_dynamic_chordal = t4_dynamic_chordal_S.encoder(t4_S)\n",
    "t4_S_H_dynamic_indicator = t4_dynamic_indicator_S.encoder(t4_S)\n",
    "\n",
    "t4_B_H_dynamic_chordal = t4_dynamic_chordal_B.encoder(t4_B)\n",
    "t4_B_H_dynamic_indicator = t4_dynamic_indicator_B.encoder(t4_B)\n",
    "\n",
    "t4_T_H_dynamic_chordal = t4_dynamic_chordal_T.encoder(t4_T)\n",
    "t4_T_H_dynamic_indicator = t4_dynamic_indicator_T.encoder(t4_T)\n",
    "\n",
    "t4_P_H_dynamic_chordal = t4_dynamic_chordal_P.encoder(t4_P)\n",
    "t4_P_H_dynamic_indicator = t4_dynamic_indicator_P.encoder(t4_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new dynamic latent space representations\n",
    "pickle.dump(t4_S_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t4_S_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t4_B_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t4_B_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t4_T_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t4_T_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t4_P_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t4_P_H_dynamic_chordal', 'wb'))\n",
    "\n",
    "pickle.dump(t4_S_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t4_S_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t4_B_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t4_B_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t4_T_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t4_T_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t4_P_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t4_P_H_dynamic_indicator', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4_dynamic_S_labels = KMeans(n_clusters=get_num_communities(t4, 'gt'), n_init=20).fit(t4_S_H_dynamic_indicator).labels_\n",
    "t4_dynamic_B_labels = KMeans(n_clusters=get_num_communities(t4, 'gt'), n_init=20).fit(t4_B_H_dynamic_indicator).labels_\n",
    "t4_dynamic_T_labels = KMeans(n_clusters=get_num_communities(t4, 'gt'), n_init=20).fit(t4_T_H_dynamic_indicator).labels_\n",
    "t4_dynamic_P_labels = KMeans(n_clusters=get_num_communities(t4, 'gt'), n_init=20).fit(t4_P_H_dynamic_indicator).labels_\n",
    "\n",
    "t4_dynamic_S_Z = create_community_indicator_matrix(labels_to_dict(t4_dynamic_S_labels), len(t4.nodes))\n",
    "t4_dynamic_B_Z = create_community_indicator_matrix(labels_to_dict(t4_dynamic_B_labels), len(t4.nodes))\n",
    "t4_dynamic_T_Z = create_community_indicator_matrix(labels_to_dict(t4_dynamic_T_labels), len(t4.nodes))\n",
    "t4_dynamic_P_Z = create_community_indicator_matrix(labels_to_dict(t4_dynamic_P_labels), len(t4.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_dynamic_chordal_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t5_dynamic_indicator_S = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t4_dynamic_S_Z)\n",
    "t5_dynamic_chordal_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t5_dynamic_indicator_B = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t4_dynamic_B_Z)\n",
    "t5_dynamic_chordal_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t5_dynamic_indicator_T = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t4_dynamic_T_Z)\n",
    "t5_dynamic_chordal_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), subspace_distance=1)\n",
    "t5_dynamic_indicator_P = Autoencoder(300, 128, k_reg=tf.keras.regularizers.L2(), act_reg=SparseRegularizer(), Z_old=t4_dynamic_P_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_dynamic_chordal_S.set_past_embedding(t4_S_H_dynamic_chordal)\n",
    "t5_dynamic_chordal_B.set_past_embedding(t4_B_H_dynamic_chordal)\n",
    "t5_dynamic_chordal_T.set_past_embedding(t4_T_H_dynamic_chordal)\n",
    "t5_dynamic_chordal_P.set_past_embedding(t4_P_H_dynamic_chordal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_t5_dynamic_chordal_S = train(t5_dynamic_chordal_S, 1000, 300, t5_S)\n",
    "history_t5_dynamic_chordal_B = train(t5_dynamic_chordal_B, 1000, 300, t5_B)\n",
    "history_t5_dynamic_chordal_T = train(t5_dynamic_chordal_T, 1000, 300, t5_T)\n",
    "history_t5_dynamic_chordal_P = train(t5_dynamic_chordal_P, 1000, 300, t5_P)\n",
    "\n",
    "history_t5_dynamic_indicator_S = train(t5_dynamic_indicator_S, 1000, 300, t5_S)\n",
    "history_t5_dynamic_indicator_B = train(t5_dynamic_indicator_B, 1000, 300, t5_B)\n",
    "history_t5_dynamic_indicator_T = train(t5_dynamic_indicator_T, 1000, 300, t5_T)\n",
    "history_t5_dynamic_indicator_P = train(t5_dynamic_indicator_P, 1000, 300, t5_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_S_H_dynamic_chordal = t5_dynamic_chordal_S.encoder(t5_S)\n",
    "t5_S_H_dynamic_indicator = t5_dynamic_indicator_S.encoder(t5_S)\n",
    "\n",
    "t5_B_H_dynamic_chordal = t5_dynamic_chordal_B.encoder(t5_B)\n",
    "t5_B_H_dynamic_indicator = t5_dynamic_indicator_B.encoder(t5_B)\n",
    "\n",
    "t5_T_H_dynamic_chordal = t5_dynamic_chordal_T.encoder(t5_T)\n",
    "t5_T_H_dynamic_indicator = t5_dynamic_indicator_T.encoder(t5_T)\n",
    "\n",
    "t5_P_H_dynamic_chordal = t5_dynamic_chordal_P.encoder(t5_P)\n",
    "t5_P_H_dynamic_indicator = t5_dynamic_indicator_P.encoder(t5_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save new dynamic latent space representations\n",
    "pickle.dump(t5_S_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t5_S_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t5_B_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t5_B_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t5_T_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t5_T_H_dynamic_chordal', 'wb'))\n",
    "pickle.dump(t5_P_H_dynamic_chordal, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_chordal/t5_P_H_dynamic_chordal', 'wb'))\n",
    "\n",
    "pickle.dump(t5_S_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t5_S_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t5_B_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t5_B_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t5_T_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t5_T_H_dynamic_indicator', 'wb'))\n",
    "pickle.dump(t5_P_H_dynamic_indicator, open('./trained/DANCer_graphs_simple_4/latent_space/dynamic_indicator/t5_P_H_dynamic_indicator', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dynamic networks\n",
    "t2_dynamic_chordal_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_dynamic_chordal_S')\n",
    "t2_dynamic_indicator_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_dynamic_indicator_S')\n",
    "t2_dynamic_chordal_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_dynamic_chordal_B')\n",
    "t2_dynamic_indicator_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_dynamic_indicator_B')\n",
    "t2_dynamic_chordal_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_dynamic_chordal_T')\n",
    "t2_dynamic_indicator_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_dynamic_indicator_T')\n",
    "t2_dynamic_chordal_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_dynamic_chordal_P')\n",
    "t2_dynamic_indicator_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t2_dynamic_indicator_P')\n",
    "\n",
    "t3_dynamic_chordal_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_dynamic_chordal_S')\n",
    "t3_dynamic_indicator_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_dynamic_indicator_S')\n",
    "t3_dynamic_chordal_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_dynamic_chordal_B')\n",
    "t3_dynamic_indicator_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_dynamic_indicator_B')\n",
    "t3_dynamic_chordal_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_dynamic_chordal_T')\n",
    "t3_dynamic_indicator_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_dynamic_indicator_T')\n",
    "t3_dynamic_chordal_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_dynamic_chordal_P')\n",
    "t3_dynamic_indicator_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t3_dynamic_indicator_P')\n",
    "\n",
    "t4_dynamic_chordal_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_dynamic_chordal_S')\n",
    "t4_dynamic_indicator_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_dynamic_indicator_S')\n",
    "t4_dynamic_chordal_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_dynamic_chordal_B')\n",
    "t4_dynamic_indicator_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_dynamic_indicator_B')\n",
    "t4_dynamic_chordal_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_dynamic_chordal_T')\n",
    "t4_dynamic_indicator_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_dynamic_indicator_T')\n",
    "t4_dynamic_chordal_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_dynamic_chordal_P')\n",
    "t4_dynamic_indicator_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t4_dynamic_indicator_P')\n",
    "\n",
    "t5_dynamic_chordal_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_dynamic_chordal_S')\n",
    "t5_dynamic_indicator_S.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_dynamic_indicator_S')\n",
    "t5_dynamic_chordal_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_dynamic_chordal_B')\n",
    "t5_dynamic_indicator_B.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_dynamic_indicator_B')\n",
    "t5_dynamic_chordal_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_dynamic_chordal_T')\n",
    "t5_dynamic_indicator_T.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_dynamic_indicator_T')\n",
    "t5_dynamic_chordal_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_dynamic_chordal_P')\n",
    "t5_dynamic_indicator_P.save('C:/Users/Owner/Documents/community_detection/trained/DANCer_graphs_simple_4/t5_dynamic_indicator_P')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "graph = t5\n",
    "ae = t5_dynamic_indicator_P\n",
    "data = t5_P\n",
    "num_iter = 10\n",
    "\n",
    "\n",
    "H = ae.encoder(data)\n",
    "labels = graph_labels(graph)\n",
    "average = 0\n",
    "for i in range(num_iter):\n",
    "    kmeans = KMeans(n_clusters=get_num_communities(graph, 'gt'), n_init=20).fit(H)\n",
    "    average += normalized_mutual_info_score(labels, kmeans.labels_)\n",
    "average /= num_iter\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
