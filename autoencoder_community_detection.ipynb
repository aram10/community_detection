{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from func_timeout import func_timeout, FunctionTimedOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to generate a bunch of graphs for training/testing, and for each, compute its modularity matrix B. We will start with LFR networks. \n",
    "\n",
    "\n",
    "Per the recommendations of [Lancichinetti et al., 2008], the paper had 1000 nodes, with an average degree of 20. The exponent of a vertex degree and the community size was -2.5 and -1.5 respectively, and the mixing parameter Î¼ was varied from 0.6 to 0.8. A \"small\" network had community sizes distributed uniformly between 10 and 50, and a \"large\" network between 20 and 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(num_vertices, avg_degree):\n",
    "    mu = random.uniform(0.6, 0.8)\n",
    "    return nx.LFR_benchmark_graph(num_vertices, 2.5, 1.5, mu, avg_degree, max_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(graph):\n",
    "    return np.asarray(nx.adjacency_matrix(graph).todense())\n",
    "\n",
    "def create_modularity_matrix(graph):\n",
    "    num_vertices = len(graph)\n",
    "    B = np.empty(shape=(num_vertices,num_vertices))\n",
    "    A = create_adjacency_matrix(graph)\n",
    "    degrees = [val for (node, val) in graph.degree()]\n",
    "    m = 0.5*sum(degrees)\n",
    "    for i in range(num_vertices):\n",
    "        for j in range(num_vertices):\n",
    "            a_ij = A[i,j]\n",
    "            k_i = degrees[i]\n",
    "            k_j = degrees[j]\n",
    "            b_ij = a_ij - (k_i * k_j * (1/2*m))\n",
    "            B[i,j] = b_ij\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_community_indicator_matrix(graph):\n",
    "    communities = {frozenset(graph.nodes[v]['community']) for v in graph}\n",
    "    num_vertices = len(graph)\n",
    "    num_communities = len(communities)\n",
    "    H = np.zeros(shape=(num_vertices, num_communities))\n",
    "    k = 0\n",
    "    for community in communities:\n",
    "        for vertex in community:\n",
    "            H[vertex, k] = 1\n",
    "        k = k + 1\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_point(num_vertices, avg_degree):\n",
    "    G = generate_network(num_vertices, avg_degree)\n",
    "    A = create_adjacency_matrix(G)\n",
    "    B = create_modularity_matrix(G)\n",
    "    H = create_community_indicator_matrix(G)\n",
    "    return (A,B,H)\n",
    "\n",
    "def generate_dataset(num_data_points, num_vertices, avg_degree):\n",
    "    data = []\n",
    "    x = 0\n",
    "    while len(data) < num_data_points:\n",
    "        dp = None\n",
    "        try:\n",
    "            dp = func_timeout(5, create_data_point, args=(num_vertices, avg_degree))\n",
    "        except:\n",
    "            continue\n",
    "        data.append(dp)\n",
    "        print(x)\n",
    "        x = x + 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(latent_dim, activation='relu'),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.Dense(input_dim, activation='sigmoid'),\n",
    "        ])\n",
    "        \n",
    "    def call(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        \n",
    "        self.B = X\n",
    "        self.H = encoded\n",
    "        self.M = decoded\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "    #def get_loss(self, X):\n",
    "        #call(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "data = generate_dataset(10, 34, 4.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
