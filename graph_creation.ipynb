{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import urllib.request as urllib\n",
    "import io\n",
    "import zipfile\n",
    "import re\n",
    "import time\n",
    "import datetime \n",
    "import dynetx as dn\n",
    "import copy\n",
    "import ast\n",
    "import glob\n",
    "import scipy.io\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.linalg import diag\n",
    "from tensorflow.keras import callbacks\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from networkx.generators.community import LFR_benchmark_graph\n",
    "from itertools import count\n",
    "from rdyn import RDyn\n",
    "from datetime import datetime\n",
    "\n",
    "from helpers import *\n",
    "from Autoencoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KARATE CLUB\n",
    "kc = nx.karate_club_graph()\n",
    "pickle.dump(kc, open(\"kc.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOOTBALL\n",
    "url = \"http://www-personal.umich.edu/~mejn/netdata/football.zip\"\n",
    "\n",
    "sock = urllib.urlopen(url)  \n",
    "s = io.BytesIO(sock.read())  \n",
    "sock.close()\n",
    "\n",
    "zf = zipfile.ZipFile(s) \n",
    "txt = zf.read(\"football.txt\").decode() \n",
    "gml = zf.read(\"football.gml\").decode()  \n",
    "gml = gml.split(\"\\n\")[1:]\n",
    "football = nx.parse_gml(gml)  \n",
    "\n",
    "pickle.dump(football, open(\"football.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LFR with mu=0.1\n",
    "lfr_1 = generate_LFR_network(1000, 0.1, 20, 100, 50, 100, 1000)\n",
    "pickle.dump(lfr_1, open(\"lfr_1.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LFR with mu=0.3\n",
    "lfr_2 = generate_LFR_network(1000, 0.3, 20, 100, 50, 100, 1000)\n",
    "pickle.dump(lfr_2, open(\"lfr_2.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LFR with mu=0.5\n",
    "lfr_3 = generate_LFR_network(1000, 0.5, 20, 100, 50, 100, 1000)\n",
    "pickle.dump(lfr_3, open(\"lfr_3.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LFR with mu=0.7\n",
    "lfr_4 = generate_LFR_network(1000, 0.7, 20, 100, 50, 100, 1000)\n",
    "pickle.dump(lfr_4, open(\"lfr_4.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORA\n",
    "cora_cites = pd.read_csv('graphs/cora_cites.csv')\n",
    "\n",
    "cora_features = pd.read_csv('graphs/cora_features.csv')\n",
    "cora_features['word_cited_id'] = cora_features['word_cited_id'].map(lambda x: x.lstrip('word'))\n",
    "cora_features['word_cited_id'] = cora_features['word_cited_id'].astype('int32')\n",
    "\n",
    "cora_labels = pd.read_csv('graphs/cora_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = nx.OrderedGraph()\n",
    "\n",
    "for index, row in cora_cites.iterrows():\n",
    "    p1 = row['cited_paper_id']\n",
    "    p2 = row['citing_paper_id']\n",
    "    c1 = list(cora_labels[cora_labels['paper_id'] == p1]['class_label'])[0]\n",
    "    c2 = list(cora_labels[cora_labels['paper_id'] == p2]['class_label'])[0]\n",
    "    f1 = {'gt': c1}\n",
    "    f2 = {'gt': c2}\n",
    "    d1 = dict.fromkeys(list(cora_features[cora_features['paper_id'] == p1]['word_cited_id']), 1)\n",
    "    d2 = dict.fromkeys(list(cora_features[cora_features['paper_id'] == p2]['word_cited_id']), 1)\n",
    "    \n",
    "    for i in range(1433):\n",
    "        f1[i] = d1.get(i+1, 0)\n",
    "        f2[i] = d2.get(i+1, 0)\n",
    "        \n",
    "    n1 = (p1, f1)\n",
    "    n2 = (p2, f2)\n",
    "    \n",
    "    cora.add_nodes_from([n1, n2])\n",
    "    cora.add_edge(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cora, open(\"cora.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building citeseer graph\n",
    "edges = pd.read_csv('graphs/citeseer-edges.csv')\n",
    "group_edges = pd.read_csv('graphs/citeseer-group-edges.csv')\n",
    "node_content = pd.read_csv('graphs/citeseer_node_features.csv', dtype=str)\n",
    "\n",
    "cs = nx.OrderedGraph()\n",
    "nodes = []\n",
    "feat_dict = {}\n",
    "\n",
    "for index, row in node_content.iterrows():\n",
    "    el = row['Node']\n",
    "    group = row['Label']\n",
    "    membership = {'gt': group}\n",
    "    features = row[1:len(node_content.columns)-1].tolist()\n",
    "    node_feat_dict = {}\n",
    "    for i in range(len(features)):\n",
    "        node_feat_dict[i] = int(features[i])\n",
    "    feat_dict[el] = node_feat_dict\n",
    "    node = (el, membership)\n",
    "    nodes.append(node)\n",
    "    \n",
    "cs.add_nodes_from(nodes)\n",
    "\n",
    "for row in edges.iterrows():\n",
    "    cs.add_edge(row[1][0], row[1][1])\n",
    "    \n",
    "nx.set_node_attributes(cs, feat_dict)\n",
    "\n",
    "pickle.dump(cs, open(\"citeseer.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  5.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RDyn Synthetic Dynamic Graph\n",
    "rdb = RDyn()\n",
    "rdb.execute(simplified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse generated graph\n",
    "\n",
    "graphNum = 99;\n",
    "direc = './results/1000_100_15_0.6_0.8_0.2_1/'\n",
    "out_file = \"./graphs/RDyn/\" + \"graph-\" + str(graphNum) + \".p\"\n",
    "\n",
    "graph = nx.OrderedGraph();\n",
    "\n",
    "gname = 'graph-' + str(graphNum) + '.txt'\n",
    "cname = 'communities-' + str(1) + '.txt'\n",
    "\n",
    "with open(direc + cname) as f:\n",
    "    mylist = list(f)\n",
    "    for line in mylist:\n",
    "        sline = str(line)\n",
    "        community = int(sline.split()[0])\n",
    "        nodes = ast.literal_eval(sline.lstrip('0123456789.- ').strip())\n",
    "        for node in nodes:\n",
    "            graph.add_nodes_from([node], gt=community)\n",
    "    f.close()\n",
    "        \n",
    "with open(direc + gname) as f:\n",
    "    mylist = list(f)\n",
    "    for line in mylist:\n",
    "        vertices = line.split();\n",
    "        v1 = int(vertices[0])\n",
    "        v2 = int(vertices[1])\n",
    "        graph.add_edges_from([(v1, v2)])\n",
    "    f.close()\n",
    "    \n",
    "with open(out_file, \"wb\") as f:\n",
    "    pickle.dump(graph, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sociopatterns high school\n",
    "\n",
    "#load csv\n",
    "thiers_df = pd.read_csv('graphs/thiers_2012.csv')\n",
    "#convert unix time to datetime\n",
    "thiers_df['timestamp'] = pd.to_datetime(thiers_df['timestamp'],unit='s')\n",
    "\n",
    "thiers_df['timestamp'] = thiers_df['timestamp'].apply(lambda dt: datetime(dt.year, dt.month, dt.day, dt.hour,30*(dt.minute // 30)))\n",
    "#load metadata\n",
    "thiers_metadata = pd.read_csv('graphs/thiers_2012_metadata.csv')\n",
    "#index by datetime\n",
    "thiers_df = thiers_df.set_index('timestamp')\n",
    "\n",
    "thiers_df = thiers_df.dropna()\n",
    "thiers_df = thiers_df.astype({'ID1': 'int32', 'ID2': 'int32'})\n",
    "\n",
    "#create base graph\n",
    "baseGraph = nx.OrderedGraph()\n",
    "l1 = list(thiers_df.drop_duplicates(subset=['ID1'])[['ID1', 'C1']].to_records(index=False))\n",
    "for el in l1:\n",
    "    m = {'gt': el[1]}\n",
    "    baseGraph.add_nodes_from([(el[0], m)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create graphs for specific school day\n",
    "day = 19\n",
    "intervals = list(thiers_df[thiers_df.index.day == day].index.unique())\n",
    "graphlist = []\n",
    "\n",
    "for interval in intervals:\n",
    "    g = copy.deepcopy(baseGraph)\n",
    "    for row in thiers_df[thiers_df.index == interval].iterrows():\n",
    "        g.add_edges_from([(row[1][0], row[1][1])])\n",
    "    graphlist.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(graphlist, open(\"thiers_day_19.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DANCer graph creation\n",
    "path = './graphs/DANCer_spicy/'\n",
    "out_path = \"./graphs/DANCer_spicy_built/\"\n",
    "\n",
    "gnames = ['t0', 't1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9']\n",
    "\n",
    "for gname in gnames:\n",
    "    graph = nx.OrderedGraph()\n",
    "    fname = gname + '.graph'\n",
    "    out_file = out_path  + gname + \".p\"\n",
    "    vertices = False;\n",
    "    edges = False;\n",
    "    with open(path + fname) as f:\n",
    "        mylist = list(f)\n",
    "        for line in mylist:\n",
    "            line = line.strip()\n",
    "            if(line == '#'):\n",
    "                continue;\n",
    "            if(line == '# Vertices'):\n",
    "                vertices = True;\n",
    "                edges = False;\n",
    "                continue;\n",
    "            if(line == '# Edges'):\n",
    "                edges = True;\n",
    "                vertices = False;\n",
    "                continue;\n",
    "            tokens = line.split(';')\n",
    "            if(vertices):\n",
    "                v = int(tokens[0])\n",
    "                atts = tokens[1].split('|')\n",
    "                com = int(tokens[len(tokens)-1])\n",
    "                node_dict = {'gt': com}\n",
    "                count = 0\n",
    "                for att in atts:\n",
    "                    node_dict[count] = float(att)\n",
    "                    count += 1\n",
    "                graph.add_nodes_from([(v, node_dict)])\n",
    "            if(edges):\n",
    "                v1 = int(tokens[0])\n",
    "                v2 = int(tokens[1])\n",
    "                graph.add_edge(v1, v2)\n",
    "        f.close()\n",
    "    with open(out_file, \"wb\") as f:\n",
    "        pickle.dump(graph, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DynEx.mat\n",
    "mat = scipy.io.loadmat('./graphs/DynEx.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat['Anew'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
